{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dd8b11-20d6-4bfe-9c88-071b3051490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from scipy.stats import chisquare, randint\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827ad256-8a33-4bd8-bc39-0b271f136d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "path_start = 'C:/Users/Aino/OneDrive - Aalto University/HY/'\n",
    "filename = path_start + 'kilpisjarvi_combined_data_10m.pkl'\n",
    "df_tab = pd.read_pickle(filename)\n",
    "\n",
    "start = datetime.now()\n",
    "print(f\"Starting processing at: {start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe0f684-6969-41c0-b8aa-c8dd96726ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tab['month'] = pd.to_datetime(df_tab['timestamp']).dt.month\n",
    "print((df_tab['month'] == 7).sum())\n",
    "\n",
    "\n",
    "# drop nan values\n",
    "columns_with_na = ['kost', 'lamp', 'kpeit', 'kkork', 'turve', 'mmaa', 'mlaji']\n",
    "df_tab = df_tab.dropna(subset=columns_with_na)\n",
    "\n",
    "\n",
    "# Rename columns\n",
    "df_tab.rename(columns={'kost': 'soil_m', 'lamp': 'soil_temp', 'kpeit': 'veg_cov',\n",
    "                  'kkork': 'veg_heig', 'turve': 'peat', 'mmaa': 'soil_thic',\n",
    "                  'mlaji': 'soil_type', 'lumip': 'snow_cov'}, inplace=True)\n",
    "\n",
    "\n",
    "# Display the counts of NaN values\n",
    "print(df_tab.isna().sum())\n",
    "print(df_tab.shape)\n",
    "\n",
    "# Define train and test data\n",
    "# Function to create train and test split for each month\n",
    "def train_test_split_by_month(df, test_size=0.33, random_state=42):\n",
    "    return train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# Assuming df_tab is your DataFrame and 'month' is the column containing month information\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "# Split each month's data separately\n",
    "for month in [6, 7, 8]:\n",
    "    df_month = df_tab[df_tab['month'] == month]\n",
    "    train_month, test_month = train_test_split_by_month(df_month)\n",
    "    train_list.append(train_month)\n",
    "    test_list.append(test_month)\n",
    "\n",
    "# Concatenate all train and test data\n",
    "train = pd.concat(train_list).reset_index(drop=True)\n",
    "test = pd.concat(test_list).reset_index(drop=True)\n",
    "\n",
    "# Check the result\n",
    "print(\"Train size:\", train.shape)\n",
    "print(\"Test size:\", test.shape)\n",
    "\n",
    "# training parameters\n",
    "train_params_sar = ['VH', 'VV', 'cos_ing', 'cslo', 'aspm', 'aspe']\n",
    "\n",
    "train_params_env = ['soil_temp', 'veg_cov', 'veg_heig', 'peat',\n",
    "                'soil_thic', 'NDVI', 'LAI','slope_deg', 'dem',\n",
    "                'aspect_deg']\n",
    "\n",
    "train_params_all = ['soil_temp', 'VH', 'VV', 'veg_cov', 'veg_heig', 'peat',\n",
    "                'soil_thic', 'NDVI', 'LAI', 'cos_ing','slope_deg',\n",
    "                'aspect_deg', 'cslo', 'aspm', 'aspe']\n",
    "\n",
    "output_var = 'soil_m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aea621-8f4a-4878-96b7-8e613abec33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# plot train and test sets\n",
    "# combine train and test sets with a marker\n",
    "train['set'] = 'train'\n",
    "test['set'] = 'test'\n",
    "\n",
    "# Combine the datasets\n",
    "combined = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "# Plot the distribution for the 'kost' variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Use distinct colors and different styles for histograms\n",
    "sns.histplot(data=combined, x='soil_m', hue='set', kde=True, palette={'train': 'blue', 'test': 'orange'}, element='step', alpha=0.6)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Distribution of Soil Moisture in Train and Test Sets')\n",
    "plt.xlabel('Soil Moisture')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Make the legend more distinct\n",
    "plt.legend(title='Dataset', loc='upper right', labels=['Train', 'Test'])\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(path_start + 'ml/distribution_kost.png')\n",
    "\n",
    "\n",
    "# Create the pair plot\n",
    "pair_plot = sns.pairplot(df_tab[['soil_m', 'VH', 'VV', 'cos_ing', 'cslo']], diag_kind='kde', markers='o', hue=None)\n",
    "\n",
    "# Show the plot\n",
    "for ax in pair_plot.axes.flatten():\n",
    "    ax.set_xlabel(ax.get_xlabel(), fontsize=16)  # Increase x-axis label size\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontsize=16)  # Increase y-axis label size\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14) \n",
    "pair_plot.savefig(path_start + 'ml/sar_param_plot.png')\n",
    "\n",
    "\n",
    "# Scatter plots for selected features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "sns.scatterplot(data=combined, x='VH', y='VV', hue='set', palette='viridis', alpha=0.7, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Scatter Plot of VH vs VV')\n",
    "axes[0, 0].set_xlim(combined['VH'].min(), combined['VH'].max())\n",
    "axes[0, 0].set_ylim(combined['VV'].min(), combined['VV'].max())\n",
    "\n",
    "sns.scatterplot(data=combined, x='cos_ing', y='cslo', hue='set', palette='viridis', alpha=0.7, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Scatter Plot of cos_ing vs cslo')\n",
    "axes[0, 1].set_xlim(combined['cos_ing'].min(), combined['cos_ing'].max())\n",
    "axes[0, 1].set_ylim(combined['cslo'].min(), combined['cslo'].max())\n",
    "\n",
    "sns.scatterplot(data=combined, x='NDVI', y='LAI', hue='set', palette='viridis', alpha=0.7, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Scatter Plot of NDVI vs LAI')\n",
    "axes[1, 0].set_xlim(combined['NDVI'].min(), combined['NDVI'].max())\n",
    "axes[1, 0].set_ylim(combined['LAI'].min(), combined['LAI'].max())\n",
    "\n",
    "sns.scatterplot(data=combined, x='kpeit', y='turve', hue='set', palette='viridis', alpha=0.7, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Scatter Plot of kpeit vs turve')\n",
    "axes[1, 1].set_xlim(combined['kpeit'].min(), combined['kpeit'].max())\n",
    "axes[1, 1].set_ylim(combined['turve'].min(), combined['turve'].max())\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(path_start + 'ml/scatter_plots_train_test.png')\n",
    "\n",
    "# Pair plot for all training parameters\n",
    "pair_plot_vars = train_params_sar + train_params_env + [output_var, 'set']\n",
    "sns.pairplot(combined[pair_plot_vars], hue='set', palette='viridis', diag_kind='kde', markers=['o', 's'])\n",
    "plt.suptitle('Pair Plot of Selected Features in Train and Test Sets', y=1.02)\n",
    "plt.savefig(path_start + 'ml/pair_plot_train.png')\n",
    "\n",
    "# Heatmap for correlation matrix\n",
    "plt.figure(figsize=(14, 10))\n",
    "correlation_matrix = combined[train_params_all + [output_var]].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='viridis')\n",
    "plt.title('Correlation Matrix of Training Parameters and Output Variable')\n",
    "plt.savefig(path_start + 'ml/correlation_train.png')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fca65ac-106f-43c8-8b3b-efe0d99d6337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate models\n",
    "def train_evaluate_model(X_train, y_train, X_test, y_test, model, title):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Combine true values and predicted values for trend line calculation\n",
    "    y_true_combined = np.concatenate([y_train, y_test])\n",
    "    y_pred_combined = np.concatenate([y_train_pred, y_test_pred])\n",
    "\n",
    "    # Calculate trend line using linear regression on combined predictions and true values\n",
    "    trend_coefficients = np.polyfit(y_pred_combined, y_true_combined, 1)\n",
    "    trend_line = np.poly1d(trend_coefficients)\n",
    "\n",
    "    # Generate trend line points\n",
    "    trend_x = np.linspace(min(y_pred_combined), max(y_pred_combined), 100)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_train_pred, y_train, c='red', alpha=0.5, label='Train')\n",
    "    plt.scatter(y_test_pred, y_test, c='blue', alpha=0.5, label='Test')\n",
    "    plt.plot(trend_x, trend_line(trend_x), color='black', linestyle='--', linewidth=3, label='Trend Line')\n",
    "    plt.xlim(0, 100)\n",
    "    plt.ylim(0, 100)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)  # Increase the tick label size\n",
    "    plt.grid()\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.ylabel('Measured Soil Moisture (VWC%)', fontsize=14)\n",
    "    plt.xlabel('Predicted Soil Moisture (VWC%)', fontsize=14)\n",
    "    plt.legend(fontsize=14)\n",
    "\n",
    "    # Evaluation metrics\n",
    "    print(f'{title} Train RMSE: {np.sqrt(mean_squared_error(y_train, y_train_pred))}')\n",
    "    print(f'{title} Test RMSE: {np.sqrt(mean_squared_error(y_test, y_test_pred))}')\n",
    "    print(f'{title} Train Corr: {np.corrcoef(y_train, y_train_pred)[0, 1]}')\n",
    "    print(f'{title} Test Corr: {np.corrcoef(y_test, y_test_pred)[0, 1]}')\n",
    "    print(f'{title} Train R²: {r2_score(y_train, y_train_pred)}')\n",
    "    print(f'{title} Test R²: {r2_score(y_test, y_test_pred)}')\n",
    "    print(f'{title} Train Explained Variance: {explained_variance_score(y_train, y_train_pred)}')\n",
    "    print(f'{title} Test Explained Variance: {explained_variance_score(y_test, y_test_pred)}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_start + f'ml/{title.replace(\" \", \"_\").lower()}.png')\n",
    "\n",
    "# Define a function to perform GridSearchCV\n",
    "def grid_search_cv(model, param_grid, X_train, y_train, model_name):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=7, verbose=1, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f\"Best {model_name} params:\", grid_search.best_params_)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Define a function to normalize data\n",
    "def normalize_data(train_df, test_df, params):\n",
    "    scaler = StandardScaler()\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(train_df[params]), columns=params)\n",
    "    y_train = train_df[output_var]\n",
    "    X_test = pd.DataFrame(scaler.transform(test_df[params]), columns=params)\n",
    "    y_test = test_df[output_var]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Common model parameters for GridSearchCV\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [500, 700, 1000],\n",
    "    'max_depth': [2, 4, 6],\n",
    "    'min_samples_split': [10, 14, 17],\n",
    "    'min_samples_leaf': [10, 13, 15],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'bootstrap': [True],\n",
    "    'max_samples': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "gb_param_grid = {\n",
    "    'learning_rate': [0.001, 0.005, 0.01],\n",
    "    'n_estimators': [500, 1000, 1500],\n",
    "    'max_depth': [2, 3],\n",
    "    'min_samples_split': [7, 10, 12],\n",
    "    'min_samples_leaf': [4, 6, 8],\n",
    "    'subsample': [0.7, 0.8],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'loss': ['huber', 'squared_error', 'absolute_error']\n",
    "    #'warm_start': [True]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b324e8-c425-42e6-a932-a1946995f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Model with SAR data\n",
    "print('TEST 1:')\n",
    "\n",
    "X_train_sar, y_train_sar, X_test_sar, y_test_sar = normalize_data(train, test, train_params_sar)\n",
    "\n",
    "rf_best_sar = grid_search_cv(RandomForestRegressor(random_state=random_seed), rf_param_grid, X_train_sar, y_train_sar, 'RF')\n",
    "gb_best_sar = grid_search_cv(GradientBoostingRegressor(random_state=random_seed), gb_param_grid, X_train_sar, y_train_sar, 'GB')\n",
    "\n",
    "# Plot and evaluate models\n",
    "train_evaluate_model(X_train_sar, y_train_sar, X_test_sar, y_test_sar, rf_best_sar, 'RF Best - SAR')\n",
    "train_evaluate_model(X_train_sar, y_train_sar, X_test_sar, y_test_sar, gb_best_sar, 'GB Best - SAR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfbe781-2edc-4f21-9298-236828542f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Model with environmental data\n",
    "print('TEST 2:')\n",
    "X_train_env, y_train_env, X_test_env, y_test_env = normalize_data(train, test, train_params_env)\n",
    "\n",
    "rf_best_env = grid_search_cv(RandomForestRegressor(random_state=random_seed), rf_param_grid, X_train_env, y_train_env, 'RF')\n",
    "gb_best_env = grid_search_cv(GradientBoostingRegressor(random_state=random_seed), gb_param_grid, X_train_env, y_train_env, 'GB')\n",
    "\n",
    "# Plot and evaluate models\n",
    "train_evaluate_model(X_train_env, y_train_env, X_test_env, y_test_env, rf_best_env, 'RF Best - ENV')\n",
    "train_evaluate_model(X_train_env, y_train_env, X_test_env, y_test_env, gb_best_env, 'GB Best - ENV')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1be99ee-ca21-4bdf-81a8-6d72f50f87f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Model with all data\n",
    "print('TEST 3:')\n",
    "X_train_all, y_train_all, X_test_all, y_test_all = normalize_data(train, test, train_params_all)\n",
    "\n",
    "rf_best_all = grid_search_cv(RandomForestRegressor(random_state=random_seed), rf_param_grid, X_train_all, y_train_all, 'RF')\n",
    "gb_best_all = grid_search_cv(GradientBoostingRegressor(random_state=random_seed), gb_param_grid, X_train_all, y_train_all, 'GB')\n",
    "\n",
    "# Plot and evaluate models\n",
    "train_evaluate_model(X_train_all, y_train_all, X_test_all, y_test_all, rf_best_all, 'RF Best - ALL')\n",
    "train_evaluate_model(X_train_all, y_train_all, X_test_all, y_test_all, gb_best_all, 'GB Best - ALL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dacb45-b3fc-4fb2-8ae3-9be30861d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances for the models\n",
    "# Random Forest Feature Importance\n",
    "def plot_feature_importance(rf_model, gb_model, feature_names, title, filename):\n",
    "    rf_importances = rf_model.feature_importances_\n",
    "    gb_importances = gb_model.feature_importances_\n",
    "\n",
    "    rf_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': rf_importances}).sort_values(by='Importance', ascending=False)\n",
    "    gb_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': gb_importances}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "\n",
    "    axes[0].barh(rf_importance_df['Feature'], rf_importance_df['Importance'])\n",
    "    axes[0].invert_yaxis()\n",
    "    axes[0].set_title('Random Forest Feature Importances', fontsize=16)\n",
    "    axes[0].set_xlabel('Relative Importance', fontsize=14)\n",
    "    axes[0].set_xlim([0, 0.5])\n",
    "    axes[0].tick_params(axis='y', labelsize=13)\n",
    "    axes[0].tick_params(axis='x', labelsize=13)\n",
    "\n",
    "    axes[1].barh(gb_importance_df['Feature'], gb_importance_df['Importance'])\n",
    "    axes[1].invert_yaxis()\n",
    "    axes[1].set_title('Gradient Boosting Feature Importances', fontsize=16)\n",
    "    axes[1].set_xlabel('Relative Importance', fontsize=12)\n",
    "    axes[1].set_xlim([0, 0.5])\n",
    "    axes[1].tick_params(axis='y', labelsize=13)\n",
    "    axes[1].tick_params(axis='x', labelsize=13)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    print(f\"Saving plot to {filename}\")\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "# Feature sets\n",
    "feature_sets = {\n",
    "    \"sar\": train_params_sar,\n",
    "    \"env\": train_params_env,\n",
    "    \"all\": train_params_all\n",
    "}\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    \"sar\": (rf_best_sar, gb_best_sar),\n",
    "    \"env\": (rf_best_env, gb_best_env),\n",
    "    \"all\": (rf_best_all, gb_best_all)\n",
    "}\n",
    "\n",
    "# Plot feature importances for each feature set and model combination\n",
    "for feature_set_name, (rf_model, gb_model) in models.items():\n",
    "    features = feature_sets[feature_set_name]\n",
    "    plot_title = f\"Feature Importances - {feature_set_name.upper()}\"\n",
    "    file_name = path_start + f\"ml/importance60_{feature_set_name}.png\"\n",
    "    plot_feature_importance(rf_model, gb_model, features, plot_title, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fbb39b-bea7-48a1-aaf4-45001ec39596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting soil moisture\n",
    "print(\"Predicting soil moisture...\")\n",
    "# Define function to prepare monthly data\n",
    "def prepare_monthly_data(monthly_df, scaler, params):\n",
    "    X_month = pd.DataFrame(scaler.transform(monthly_df[params]), columns=params)\n",
    "    return X_month\n",
    "    \n",
    "# Define function to make maps of the actual soil moisture values\n",
    "\n",
    "def plot_actual_kost(monthly_df, lat, lon, title, filename, lat_bins=30, lon_bins=30):\n",
    "    kost_values = monthly_df['soil_m']\n",
    "\n",
    "    # Create a grid based on latitude and longitude\n",
    "    lat_edges = np.linspace(lat.min(), lat.max(), lat_bins + 1)\n",
    "    lon_edges = np.linspace(lon.min(), lon.max(), lon_bins + 1)\n",
    "    lat_centers = 0.5 * (lat_edges[:-1] + lat_edges[1:])\n",
    "    lon_centers = 0.5 * (lon_edges[:-1] + lon_edges[1:])\n",
    "\n",
    "    # Create a 2D histogram to count the number of points in each bin\n",
    "    grid, _, _ = np.histogram2d(lat, lon, bins=[lat_edges, lon_edges], weights=kost_values)\n",
    "\n",
    "    # Normalize the grid by the number of points in each bin\n",
    "    counts, _, _ = np.histogram2d(lat, lon, bins=[lat_edges, lon_edges])\n",
    "    grid = np.divide(grid, counts, where=counts != 0)\n",
    "    grid[counts == 0] = np.nan\n",
    "\n",
    "    # Set up the Basemap\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # Plot the data on the map\n",
    "    mesh = ax.pcolormesh(lon_centers, lat_centers, grid.T, shading='auto', cmap='BrBG', vmin=0, vmax=80)\n",
    "\n",
    "    cbar = plt.colorbar(mesh, ax=ax)\n",
    "    cbar.set_label('Soil Moisture (VWC%)', fontsize=14)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Longitude', fontsize=14)\n",
    "    plt.ylabel('Latitude', fontsize=14)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)  # Increase the tick label size\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "def make_predictions(model, X_month, lat, lon, title, filename,  lat_bins=30, lon_bins=30):\n",
    "    y_month_pred = model.predict(X_month)\n",
    "\n",
    "    # Create a grid based on latitude and longitude\n",
    "    lat_edges = np.linspace(lat.min(), lat.max(), lat_bins + 1)\n",
    "    lon_edges = np.linspace(lon.min(), lon.max(), lon_bins + 1)\n",
    "    lat_centers = 0.5 * (lat_edges[:-1] + lat_edges[1:])\n",
    "    lon_centers = 0.5 * (lon_edges[:-1] + lon_edges[1:])\n",
    "\n",
    "    # Create a 2D histogram to count the number of points in each bin\n",
    "    grid, _, _ = np.histogram2d(lat, lon, bins=[lat_edges, lon_edges], weights=y_month_pred)\n",
    "    counts, _, _ = np.histogram2d(lat, lon, bins=[lat_edges, lon_edges])\n",
    "    grid = np.divide(grid, counts, where=counts != 0)\n",
    "    grid[counts == 0] = np.nan\n",
    "\n",
    "    # Set up the Basemap\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    # Plot the data on the map\n",
    "    mesh = ax.pcolormesh(lon_centers, lat_centers, grid.T, shading='auto', cmap='BrBG', vmin=0, vmax=80)\n",
    "\n",
    "    cbar = plt.colorbar(mesh, ax=ax)\n",
    "    cbar.set_label('Soil Moisture (VWC%)', fontsize=14)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Longitude', fontsize=14)\n",
    "    plt.ylabel('Latitude', fontsize=14)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)  # Increase the tick label size\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e41e3ab-042e-4665-a096-99bb481061a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly data\n",
    "monthly_data_june = df_tab[df_tab['month'] == 6]\n",
    "monthly_data_july = df_tab[df_tab['month'] == 7]\n",
    "monthly_data_august = df_tab[df_tab['month'] == 8]\n",
    "\n",
    "test_june_sar = test[test['month'] == 6][train_params_sar]\n",
    "test_july_sar = test[test['month'] == 7][train_params_sar]\n",
    "test_august_sar = test[test['month'] == 8][train_params_sar]\n",
    "\n",
    "test_june_env = test[test['month'] == 6][train_params_env]\n",
    "test_july_env = test[test['month'] == 7][train_params_env]\n",
    "test_august_env = test[test['month'] == 8][train_params_env]\n",
    "\n",
    "test_june_all = test[test['month'] == 6][train_params_all]\n",
    "test_july_all = test[test['month'] == 7][train_params_all]\n",
    "test_august_all = test[test['month'] == 8][train_params_all]\n",
    "\n",
    "# Normalize the data using the same scaler\n",
    "scaler_sar = StandardScaler().fit(train[train_params_sar])\n",
    "scaler_env = StandardScaler().fit(train[train_params_env])\n",
    "scaler_all = StandardScaler().fit(train[train_params_all])\n",
    "\n",
    "# Prepare the data for each month\n",
    "X_test_june_sar = prepare_monthly_data(test_june_sar, scaler_sar, train_params_sar)\n",
    "X_test_july_sar = prepare_monthly_data(test_july_sar, scaler_sar, train_params_sar)\n",
    "X_test_august_sar = prepare_monthly_data(test_august_sar, scaler_sar, train_params_sar)\n",
    "\n",
    "X_test_june_env = prepare_monthly_data(test_june_env, scaler_env, train_params_env)\n",
    "X_test_july_env = prepare_monthly_data(test_july_env, scaler_env, train_params_env)\n",
    "X_test_august_env = prepare_monthly_data(test_august_env, scaler_env, train_params_env)\n",
    "\n",
    "X_test_june_all = prepare_monthly_data(test_june_all, scaler_all, train_params_all)\n",
    "X_test_july_all = prepare_monthly_data(test_july_all, scaler_all, train_params_all)\n",
    "X_test_august_all = prepare_monthly_data(test_august_all, scaler_all, train_params_all)\n",
    "\n",
    "\n",
    "# Make predictions and plot the results\n",
    "\n",
    "# SAR Data\n",
    "make_predictions(rf_best_sar, X_test_june_sar, test[test['month'] == 6]['lat'], test[test['month'] == 6]['lon'], 'RF SAR Predictions - June', path_start + 'ml/predictions_rf_sar_june.png')\n",
    "make_predictions(rf_best_sar, X_test_july_sar, test[test['month'] == 7]['lat'], test[test['month'] == 7]['lon'], 'RF SAR Predictions - July', path_start + 'ml/predictions_rf_sar_july.png')\n",
    "make_predictions(rf_best_sar, X_test_august_sar, test[test['month'] == 8]['lat'], test[test['month'] == 8]['lon'], 'RF SAR Predictions - August', path_start + 'ml/predictions_rf_sar_august.png')\n",
    "\n",
    "make_predictions(gb_best_sar, X_test_june_sar, test[test['month'] == 6]['lat'], test[test['month'] == 6]['lon'], 'GB SAR Predictions - June', path_start + 'ml/predictions_gb_sar_june.png')\n",
    "make_predictions(gb_best_sar, X_test_july_sar, test[test['month'] == 7]['lat'], test[test['month'] == 7]['lon'], 'GB SAR Predictions - July', path_start + 'ml/predictions_gb_sar_july.png')\n",
    "make_predictions(gb_best_sar, X_test_august_sar, test[test['month'] == 8]['lat'], test[test['month'] == 8]['lon'], 'GB SAR Predictions - August', path_start + 'ml/predictions_gb_sar_august.png')\n",
    "\n",
    "# Environmental Data\n",
    "make_predictions(rf_best_env, X_test_june_env, test[test['month'] == 6]['lat'], test[test['month'] == 6]['lon'], 'RF ENV Predictions - June', path_start + 'ml/predictions_rf_env_june.png')\n",
    "make_predictions(rf_best_env, X_test_july_env, test[test['month'] == 7]['lat'], test[test['month'] == 7]['lon'], 'RF ENV Predictions - July', path_start + 'ml/predictions_rf_env_july.png')\n",
    "make_predictions(rf_best_env, X_test_august_env, test[test['month'] == 8]['lat'], test[test['month'] == 8]['lon'], 'RF ENV Predictions - August', path_start + 'ml/predictions_rf_env_august.png')\n",
    "\n",
    "make_predictions(gb_best_env, X_test_june_env, test[test['month'] == 6]['lat'], test[test['month'] == 6]['lon'], 'GB ENV Predictions - June', path_start + 'ml/predictions_gb_env_june.png')\n",
    "make_predictions(gb_best_env, X_test_july_env, test[test['month'] == 7]['lat'], test[test['month'] == 7]['lon'], 'GB ENV Predictions - July', path_start + 'ml/predictions_gb_env_july.png')\n",
    "make_predictions(gb_best_env, X_test_august_env, test[test['month'] == 8]['lat'], test[test['month'] == 8]['lon'], 'GB ENV Predictions - August', path_start + 'ml/predictions_gb_env_august.png')\n",
    "\n",
    "# All Data\n",
    "make_predictions(rf_best_all, X_test_june_all, test[test['month'] == 6]['lat'], test[test['month'] == 6]['lon'], 'RF ALL Predictions - June', path_start + 'ml/predictions_rf_all_june.png')\n",
    "make_predictions(rf_best_all, X_test_july_all, test[test['month'] == 7]['lat'], test[test['month'] == 7]['lon'], 'RF ALL Predictions - July', path_start + 'ml/predictions_rf_all_july.png')\n",
    "make_predictions(rf_best_all, X_test_august_all, test[test['month'] == 8]['lat'], test[test['month'] == 8]['lon'], 'RF ALL Predictions - August', path_start + 'ml/predictions_rf_all_august.png')\n",
    "\n",
    "make_predictions(gb_best_all, X_test_june_all, test[test['month'] == 6]['lat'], test[test['month'] == 6]['lon'], 'GB ALL Predictions - June', path_start + 'ml/predictions_gb_all_june.png')\n",
    "make_predictions(gb_best_all, X_test_july_all, test[test['month'] == 7]['lat'], test[test['month'] == 7]['lon'], 'GB ALL Predictions - July', path_start + 'ml/predictions_gb_all_july.png')\n",
    "make_predictions(gb_best_all, X_test_august_all, test[test['month'] == 8]['lat'], test[test['month'] == 8]['lon'], 'GB ALL Predictions - August', path_start + 'ml/predictions_gb_all_august.png')\n",
    "\n",
    "# Plot actual kost values\n",
    "plot_actual_kost(test[test['month'] == 6], test[test['month'] == 6]['lat'], test[test['month'] == 6]['lon'], 'Measured Soil Moisture (kost) - June', path_start + 'ml/test_actual_kost_june.png')\n",
    "plot_actual_kost(test[test['month'] == 7], test[test['month'] == 7]['lat'], test[test['month'] == 7]['lon'], 'Measured Soil Moisture (kost) - July', path_start + 'ml/test_actual_kost_july.png')\n",
    "plot_actual_kost(test[test['month'] == 8], test[test['month'] == 8]['lat'], test[test['month'] == 8]['lon'], 'Measured Soil Moisture (kost) - August', path_start + 'ml/test_actual_kost_august.png')\n",
    "\n",
    "# Processing end\n",
    "end = datetime.now()\n",
    "print(f\"Finished processing at: {end}\")\n",
    "processing_time = end - start\n",
    "print(f\"Processing took: {processing_time.total_seconds() / 3600} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ba6e62-f423-4845-841d-a89712501900",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_predictions(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, mse, rmse, r2\n",
    "    \n",
    "# Define a function to make predictions and evaluate them\n",
    "def make_and_evaluate_predictions(model, X_month, y_true, month, model_name, feature_set):\n",
    "    y_pred = model.predict(X_month)\n",
    "    mae, mse, rmse, r2 = evaluate_predictions(y_true, y_pred)\n",
    "    print(f\"{model_name} Predictions ({feature_set}) - {month}: MAE={mae}, MSE={mse}, RMSE={rmse}, R2={r2}\")\n",
    "    return mae, mse, rmse, r2\n",
    "\n",
    "# Define the actual values for evaluation\n",
    "y_june_actual = test[test['month'] == 6][output_var].values\n",
    "y_july_actual = test[test['month'] == 7][output_var].values\n",
    "y_august_actual = test[test['month'] == 8][output_var].values\n",
    "\n",
    "# Models and feature sets\n",
    "models = {\n",
    "    'RF': {'sar': rf_best_sar, 'env': rf_best_env, 'all': rf_best_all},\n",
    "    'GB': {'sar': gb_best_sar, 'env': gb_best_env, 'all': gb_best_all}\n",
    "}\n",
    "\n",
    "feature_sets = {\n",
    "    'sar': {'june': X_test_june_sar, 'july': X_test_july_sar, 'august': X_test_august_sar},\n",
    "    'env': {'june': X_test_june_env, 'july': X_test_july_env, 'august': X_test_august_env},\n",
    "    'all': {'june': X_test_june_all, 'july': X_test_july_all, 'august': X_test_august_all}\n",
    "}\n",
    "\n",
    "actual_values = {\n",
    "    'june': y_june_actual,\n",
    "    'july': y_july_actual,\n",
    "    'august': y_august_actual\n",
    "}\n",
    "\n",
    "# Evaluate all models and feature sets for each month\n",
    "for model_name, model_dict in models.items():\n",
    "    for feature_set, model in model_dict.items():\n",
    "        for month, X_month in feature_sets[feature_set].items():\n",
    "            y_true = actual_values[month]\n",
    "            make_and_evaluate_predictions(model, X_month, y_true, month, model_name, feature_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3207ad8b-c458-4b95-87f0-afe5e7c4c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For train data (just for comparison)\n",
    "# Normalize the data using the same scaler\n",
    "scaler_sar = StandardScaler().fit(train[train_params_sar])\n",
    "scaler_env = StandardScaler().fit(train[train_params_env])\n",
    "scaler_all = StandardScaler().fit(train[train_params_all])\n",
    "\n",
    "\n",
    "train_june_sar = train[train['month'] == 6][train_params_sar]\n",
    "train_july_sar = train[train['month'] == 7][train_params_sar]\n",
    "train_august_sar = train[train['month'] == 8][train_params_sar]\n",
    "\n",
    "train_june_env = train[train['month'] == 6][train_params_env]\n",
    "train_july_env = train[train['month'] == 7][train_params_env]\n",
    "train_august_env = train[train['month'] == 8][train_params_env]\n",
    "\n",
    "train_june_all = train[train['month'] == 6][train_params_all]\n",
    "train_july_all = train[train['month'] == 7][train_params_all]\n",
    "train_august_all = train[train['month'] == 8][train_params_all]\n",
    "\n",
    "# Prepare the data for each month\n",
    "X_train_june_sar = prepare_monthly_data(train_june_sar, scaler_sar, train_params_sar)\n",
    "X_train_july_sar = prepare_monthly_data(train_july_sar, scaler_sar, train_params_sar)\n",
    "X_train_august_sar = prepare_monthly_data(train_august_sar, scaler_sar, train_params_sar)\n",
    "\n",
    "X_train_june_env = prepare_monthly_data(train_june_env, scaler_env, train_params_env)\n",
    "X_train_july_env = prepare_monthly_data(train_july_env, scaler_env, train_params_env)\n",
    "X_train_august_env = prepare_monthly_data(train_august_env, scaler_env, train_params_env)\n",
    "\n",
    "X_train_june_all = prepare_monthly_data(train_june_all, scaler_all, train_params_all)\n",
    "X_train_july_all = prepare_monthly_data(train_july_all , scaler_all, train_params_all)\n",
    "X_train_august_all = prepare_monthly_data(train_august_all, scaler_all, train_params_all)\n",
    "\n",
    "\n",
    "# Make predictions and plot the results\n",
    "\n",
    "# SAR Data\n",
    "make_predictions(rf_best_sar, X_train_june_sar, train[train['month'] == 6]['lat'], train[train['month'] == 6]['lon'], 'RF SAR Predictions - June', path_start + 'ml/predictions_rf_sar_june2.png')\n",
    "make_predictions(rf_best_sar, X_train_july_sar, train[train['month'] == 7]['lat'], train[train['month'] == 7]['lon'], 'RF SAR Predictions - July', path_start + 'ml/predictions_rf_sar_july2.png')\n",
    "make_predictions(rf_best_sar, X_train_august_sar, train[train['month'] == 8]['lat'], train[train['month'] == 8]['lon'], 'RF SAR Predictions - August', path_start + 'ml/predictions_rf_sar_august2.png')\n",
    "\n",
    "make_predictions(gb_best_sar, X_train_june_sar, train[train['month'] == 6]['lat'], train[train['month'] == 6]['lon'], 'GB SAR Predictions - June', path_start + 'ml/predictions_gb_sar_june2.png')\n",
    "make_predictions(gb_best_sar, X_train_july_sar, train[train['month'] == 7]['lat'], train[train['month'] == 7]['lon'], 'GB SAR Predictions - July', path_start + 'ml/predictions_gb_sar_july2.png')\n",
    "make_predictions(gb_best_sar, X_train_august_sar, train[train['month'] == 8]['lat'], train[train['month'] == 8]['lon'], 'GB SAR Predictions - August', path_start + 'ml/predictions_gb_sar_august2.png')\n",
    "\n",
    "# Environmental Data\n",
    "make_predictions(rf_best_env, X_train_june_env, train[train['month'] == 6]['lat'], train[train['month'] == 6]['lon'], 'RF ENV Predictions - June', path_start + 'ml/predictions_rf_env_june2.png')\n",
    "make_predictions(rf_best_env, X_train_july_env, train[train['month'] == 7]['lat'], train[train['month'] == 7]['lon'], 'RF ENV Predictions - July', path_start + 'ml/predictions_rf_env_july2.png')\n",
    "make_predictions(rf_best_env, X_train_august_env, train[train['month'] == 8]['lat'], train[train['month'] == 8]['lon'], 'RF ENV Predictions - August', path_start + 'ml/predictions_rf_env_august2.png')\n",
    "\n",
    "make_predictions(gb_best_env, X_train_june_env, train[train['month'] == 6]['lat'], train[train['month'] == 6]['lon'], 'GB ENV Predictions - June', path_start + 'ml/predictions_gb_env_june2.png')\n",
    "make_predictions(gb_best_env, X_train_july_env, train[train['month'] == 7]['lat'], train[train['month'] == 7]['lon'], 'GB ENV Predictions - July', path_start + 'ml/predictions_gb_env_july2.png')\n",
    "make_predictions(gb_best_env, X_train_august_env, train[train['month'] == 8]['lat'], train[train['month'] == 8]['lon'], 'GB ENV Predictions - August', path_start + 'ml/predictions_gb_env_august2.png')\n",
    "\n",
    "# All Data\n",
    "make_predictions(rf_best_all, X_train_june_all, train[train['month'] == 6]['lat'], train[train['month'] == 6]['lon'], 'RF ALL Predictions - June', path_start + 'ml/predictions_rf_all_june2.png')\n",
    "make_predictions(rf_best_all, X_train_july_all, train[train['month'] == 7]['lat'], train[train['month'] == 7]['lon'], 'RF ALL Predictions - July', path_start + 'ml/predictions_rf_all_july2.png')\n",
    "make_predictions(rf_best_all, X_train_august_all, train[train['month'] == 8]['lat'], train[train['month'] == 8]['lon'], 'RF ALL Predictions - August', path_start + 'ml/predictions_rf_all_august2.png')\n",
    "\n",
    "make_predictions(gb_best_all, X_train_june_all, train[train['month'] == 6]['lat'], train[train['month'] == 6]['lon'], 'GB ALL Predictions - June', path_start + 'ml/predictions_gb_all_june2.png')\n",
    "make_predictions(gb_best_all, X_train_july_all, train[train['month'] == 7]['lat'], train[train['month'] == 7]['lon'], 'GB ALL Predictions - July', path_start + 'ml/predictions_gb_all_july2.png')\n",
    "make_predictions(gb_best_all, X_train_august_all, train[train['month'] == 8]['lat'], train[train['month'] == 8]['lon'], 'GB ALL Predictions - August', path_start + 'ml/predictions_gb_all_august2.png')\n",
    "\n",
    "# Plot actual kost values\n",
    "plot_actual_kost(train[train['month'] == 6], train[train['month'] == 6]['lat'], train[train['month'] == 6]['lon'], 'Measured Soil Moisture (kost) - June', path_start + 'ml/train_actual_kost_june.png')\n",
    "plot_actual_kost(train[train['month'] == 7], train[train['month'] == 7]['lat'], train[train['month'] == 7]['lon'], 'Measured Soil Moisture (kost) - July', path_start + 'ml/train_actual_kost_july.png')\n",
    "plot_actual_kost(train[train['month'] == 8], train[train['month'] == 8]['lat'], train[train['month'] == 8]['lon'], 'Measured Soil Moisture (kost) - August', path_start + 'ml/train_actual_kost_august.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14c00da-6241-4b0a-95c5-3a976f3c8d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the actual values for evaluation\n",
    "y_june_actual = train[train['month'] == 6][output_var].values\n",
    "y_july_actual = train[train['month'] == 7][output_var].values\n",
    "y_august_actual = train[train['month'] == 8][output_var].values\n",
    "\n",
    "# Models and feature sets\n",
    "models = {\n",
    "    'RF': {'sar': rf_best_sar, 'env': rf_best_env, 'all': rf_best_all},\n",
    "    'GB': {'sar': gb_best_sar, 'env': gb_best_env, 'all': gb_best_all}\n",
    "}\n",
    "\n",
    "feature_sets = {\n",
    "    'sar': {'june': X_train_june_sar, 'july': X_train_july_sar, 'august': X_train_august_sar},\n",
    "    'env': {'june': X_train_june_env, 'july': X_train_july_env, 'august': X_train_august_env},\n",
    "    'all': {'june': X_train_june_all, 'july': X_train_july_all, 'august': X_train_august_all}\n",
    "}\n",
    "\n",
    "actual_values = {\n",
    "    'june': y_june_actual,\n",
    "    'july': y_july_actual,\n",
    "    'august': y_august_actual\n",
    "}\n",
    "\n",
    "# Evaluate all models and feature sets for each month\n",
    "for model_name, model_dict in models.items():\n",
    "    for feature_set, model in model_dict.items():\n",
    "        for month, X_month in feature_sets[feature_set].items():\n",
    "            y_true = actual_values[month]\n",
    "            make_and_evaluate_predictions(model, X_month, y_true, month, model_name, feature_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56316bd3-664f-414d-b089-cb1bdaa31e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
